{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addit(lis,ins):\n",
    "    if len(lis)<10:\n",
    "        lis.append(ins)\n",
    "def formatted(article):\n",
    "    words = 0\n",
    "    split = 0\n",
    "    for i in range(len(article.text)):\n",
    "        if words >100 and article.text[i]==\"\\n\":\n",
    "            text = article.text[split:i]\n",
    "            break\n",
    "        if article.text[i] == ' ':\n",
    "            words+=1\n",
    "        if (article.text[i] =='\\n' or article.text[i] ==')')  and words < 4:\n",
    "            split = i+1\n",
    "    else:\n",
    "        text = article.text\n",
    "    fulltext= \"Title \\n \"+article.title+'\\n\\n'+text+'\\n\\n Read more at '+article.url+'\\n\\n\\n\\n'\n",
    "    return fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Times\n",
      "Building NDTV\n",
      "Building Telegrph\n",
      "Building CNN\n",
      "All newspapers built\n"
     ]
    }
   ],
   "source": [
    "print(\"Building Times\")\n",
    "times = newspaper.build('https://www.indiatimes.com', memoize_articles=False )\n",
    "print(\"Building NDTV\")\n",
    "ndtv = newspaper.build('https://sports.ndtv.com', memoize_articles=False )\n",
    "print(\"Building Telegrph\")\n",
    "telegraph = newspaper.build('https://www.telegraphindia.com', memoize_articles=False )\n",
    "print(\"Building CNN\")\n",
    "cnn = newspaper.build('https://www.cnn.com', memoize_articles=False )\n",
    "print(\"All newspapers built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorizing articles\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorizing articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen={'times':[],\"telegraph\":[],\"cnn\":[]}\n",
    "time_paper ={}\n",
    "in_news=[]\n",
    "w_news=[]\n",
    "weird=[]\n",
    "sports=[]\n",
    "tech_n=[]\n",
    "tech_g=[]\n",
    "celebs=[]\n",
    "for article in times.articles:\n",
    "    if re.match(r'^https://www\\.indiatimes\\.com/news/india/[a-z]+',article.url) :\n",
    "        addit(in_news,article)\n",
    "    elif re.match(r'^https://www\\.indiatimes\\.com/news/world/[a-z]+',article.url):\n",
    "        addit(w_news,article)\n",
    "    elif re.match(r'^https://www\\.indiatimes\\.com/news/weird/[a-z]+',article.url):\n",
    "        addit(weird,article)\n",
    "    elif re.match(r'^https://www\\.indiatimes\\.com/news/sports/[a-z]+',article.url):\n",
    "        addit(sports,article)\n",
    "    elif re.match(r'^https://www\\.indiatimes\\.com/technology/news/[a-z]+',article.url):\n",
    "        addit(tech_n,article)\n",
    "    elif re.match(r'^https://www\\.indiatimes\\.com/technology/gadgets/[a-z]+',article.url):\n",
    "        addit(tech_g,article)\n",
    "    elif re.match(r'^https://www\\.indiatimes\\.com/entertainment/celebs/[a-z]+',article.url):\n",
    "        addit(celebs,article)\n",
    "        \n",
    "time_paper[\"in_news\"] = in_news\n",
    "time_paper[\"w_news\"] = w_news\n",
    "time_paper[\"weird\"] = weird\n",
    "time_paper[\"sports\"] = sports\n",
    "time_paper[\"tech_n\"] = tech_n\n",
    "time_paper[\"tech_g\"] = tech_g\n",
    "time_paper[\"celebs\"] = celebs\n",
    "chosen['times'] = time_paper        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndtv_paper ={}\n",
    "ipl=[]\n",
    "commonwealth=[]\n",
    "epl=[]\n",
    "football=[]\n",
    "cricket=[]\n",
    "tennis=[]\n",
    "hockey=[]\n",
    "badminton=[]\n",
    "for article in ndtv.articles:\n",
    "    if re.match(r'https://sports\\.ndtv\\.com/commonwealth-games-2018/[a-z]+',article.url) :\n",
    "        addit(commonwealth,article)\n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/indian-premier-league-2018/[a-z]+',article.url) :\n",
    "        addit(ipl,article)\n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/english-premier-league/[a-z]+',article.url) :\n",
    "        addit(epl,article)\n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/football/[a-z]+',article.url) :\n",
    "        addit(football,article)\n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/cricket/[a-z]+',article.url) :\n",
    "        addit(cricket,article)        \n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/tennis/[a-z]+',article.url) :\n",
    "        addit(tennis,article)\n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/hockey/[a-z]+',article.url) :\n",
    "        addit(hockey,article)\n",
    "    elif re.match(r'https://sports\\.ndtv\\.com/badminton/[a-z]+',article.url) :\n",
    "        addit(badminton,article)\n",
    "        \n",
    "ndtv_paper[\"ipl\"] = ipl\n",
    "ndtv_paper[\"commonwealth\"] = commonwealth\n",
    "ndtv_paper[\"epl\"] = epl\n",
    "ndtv_paper[\"football\"] = football\n",
    "ndtv_paper[\"cricket\"] = cricket\n",
    "ndtv_paper[\"tennis\"] = tennis\n",
    "ndtv_paper[\"hockey\"] = hockey\n",
    "ndtv_paper[\"badminton\"] = badminton\n",
    "chosen['Ndtv'] = ndtv_paper   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndtv_paper ={}\n",
    "india=[]\n",
    "world=[]\n",
    "business=[]\n",
    "sports=[]\n",
    "science=[]\n",
    "entertainment=[]\n",
    "for article in telegraph.articles:\n",
    "    if re.match(r'^https://www\\.telegraphindia\\.com/india/[a-z]+',article.url) :\n",
    "        addit(india,article)\n",
    "    elif re.match(r'^https://www\\.telegraphindia\\.com/business/[a-z]+',article.url):\n",
    "        addit(business,article)\n",
    "    elif re.match(r'^https://www\\.telegraphindia\\.com/world/[a-z]+',article.url):\n",
    "        addit(world,article)\n",
    "    elif re.match(r'^https://www\\.telegraphindia\\.com/sports/[a-z]+',article.url):\n",
    "        addit(sports,article)\n",
    "    elif re.match(r'^https://www\\.telegraphindia\\.com/science/[a-z]+',article.url):\n",
    "        addit(science,article)\n",
    "    elif re.match(r'^https://www\\.telegraphindia\\.com/entertainment/[a-z]+',article.url):\n",
    "        addit(entertainment,article)\n",
    "        \n",
    "tele_paper[\"india\"] = india\n",
    "tele_paper[\"world\"] = world\n",
    "tele_paper[\"business\"] = business\n",
    "tele_paper[\"sports\"] = sports\n",
    "tele_paper[\"science\"] = science\n",
    "tele_paper[\"entertainment\"] = entertainment\n",
    "chosen['telegraph'] = tele_paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_paper ={}\n",
    "entertainment=[]\n",
    "middle_east=[]\n",
    "china=[]\n",
    "africa=[]\n",
    "politics=[]\n",
    "europe=[]\n",
    "us=[]\n",
    "asia=[]\n",
    "for article in cnn.articles:  \n",
    "    if re.match(r'.*/entertainment/.+',article.url) :\n",
    "        addit(entertainment,article)\n",
    "    elif re.match(r'.*/middle-east/.+',article.url):\n",
    "        addit(middle_east,article)\n",
    "    elif re.match(r'.*/china/.+',article.url):\n",
    "        addit(china,article)\n",
    "    elif re.match(r'.*/africa/.+',article.url):\n",
    "        addit(africa,article)\n",
    "    elif re.match(r'.*/politics/.+',article.url):\n",
    "        addit(politics,article)\n",
    "    elif re.match(r'.*/europe/.+',article.url):\n",
    "        addit(europe,article)\n",
    "    elif re.match(r'.*/us/.+',article.url):\n",
    "        addit(us,article)\n",
    "    elif re.match(r'.*/asia/.+',article.url):       \n",
    "        addit(asia,article)\n",
    "        \n",
    "cnn_paper[\"entertainment\"] =entertainment\n",
    "cnn_paper[\"middle-east\"] =middle_east \n",
    "cnn_paper[\"china\"] =china\n",
    "cnn_paper[\"africa\"] =africa\n",
    "cnn_paper[\"politics\"] =politics\n",
    "cnn_paper[\"europe\"] =europe\n",
    "cnn_paper[\"us\"] =us\n",
    "cnn_paper[\"asia\"] =asia\n",
    "chosen['cnn'] = cnn_paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorization done\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorization done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "if os.path.exists('news'):\n",
    "    shutil.rmtree('/news', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading articles from times\n",
      "All articles from times downloaded\n",
      "Downloading articles from telegraph\n",
      "All articles from telegraph downloaded\n",
      "Downloading articles from cnn\n",
      "All articles from cnn downloaded\n",
      "Downloading articles from Ndtv\n",
      "All articles from Ndtv downloaded\n",
      "Kaam ho gaya\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import urllib\n",
    "import requests\n",
    "title=[]\n",
    "for key, value in chosen.items():\n",
    "    print(\"Downloading articles from \"+key)\n",
    "    for key1,value1 in value.items():\n",
    "        art_no =1\n",
    "        path= 'news/'+key+'/'+key1+'/'\n",
    "        os.makedirs(path)\n",
    "        with io.open(path+key1+'.txt','w', encoding=\"utf-8\") as file:\n",
    "            text_towrite=''\n",
    "            for article in value1:\n",
    "                article.download()\n",
    "                time.sleep(1)\n",
    "                article.parse()\n",
    "                time.sleep(0.1)\n",
    "                if article.title not in title:\n",
    "                    text_towrite +=\"Article no \"+str(art_no)+\"\\n\"+ formatted(article)\n",
    "                    f = open(path+\"Article no \"+str(art_no)+\".jpg\",'wb')\n",
    "                    f.write(requests.get(article.top_image).content)\n",
    "                    f.close()\n",
    "                    art_no+=1\n",
    "                    title.append(article.title)\n",
    "            file.write(text_towrite)    \n",
    "            file.close()\n",
    "    print('All articles from '+ key + ' downloaded')\n",
    "print('Kaam ho gaya')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
